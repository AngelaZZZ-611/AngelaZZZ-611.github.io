---
---
@article{long2023distance,
  title={Long-distance Targeted Poisoning Attacks on Graph Neural Networks},
  author={Zhang, Anqi and Panda, Aurojit and Li, Jinyang and Sen, Siddhartha},
  journal={Preprints, },
  year={2023},
  preview={long-distance-attacks.png}
}


@InProceedings{pmlr-v162-lin22h,
  title = 	 {Measuring the Effect of Training Data on Deep Learning Predictions via Randomized Experiments},
  author =       {Zhang, Anqi and Lin, Jinkun and L{\'e}cuyer, Mathias and Li, Jinyang and Panda, Aurojit and Sen, Siddhartha},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {13468--13504},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/lin22h/lin22h.pdf},
  url = 	 {https://proceedings.mlr.press/v162/lin22h.html},
  abstract = 	 {We develop a new, principled algorithm for estimating the contribution of training data points to the behavior of a deep learning model, such as a specific prediction it makes. Our algorithm estimates the AME, a quantity that measures the expected (average) marginal effect of adding a data point to a subset of the training data, sampled from a given distribution. When subsets are sampled from the uniform distribution, the AME reduces to the well-known Shapley value. Our approach is inspired by causal inference and randomized experiments: we sample different subsets of the training data to train multiple submodels, and evaluate each submodel’s behavior. We then use a LASSO regression to jointly estimate the AME of each data point, based on the subset compositions. Under sparsity assumptions ($k \ll N$ datapoints have large AME), our estimator requires only $O(k\log N)$ randomized submodel trainings, improving upon the best prior Shapley value estimators.},
  preview={enola-illustration.png}
}

@article{https://doi.org/10.1002/cpe.3913,
abbr={SCI},
author = {Yao, Haipeng and Yang, Hao and Zhang, Anqi and Fang, Chao and Guo, Yiru},
title = {WLAN interference self-optimization using som neural networks},
journal = {Concurrency and Computation: Practice and Experience},
volume = {29},
number = {3},
pages = {e3913},
keywords = {self-organizing feature map, wireless access point, interference, neural network},
doi = {https://doi.org/10.1002/cpe.3913},
pdf = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.3913},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.3913},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpe.3913},
note = {e3913 cpe.3913},
abstract = {Summary In order to suppress the interference in local area networks, this paper presents a Wireless Local Area Networks (WLAN) interference self-optimization method based on a Self-Organizing Feature Map (SOM) neural network model. This method trains the model by using original data sets as the initial vector set and using the whole Signal to Interference plus Noise Ratio (SINR) vector generated by the change of one Wireless Access Point (AP) channel as the basic feature. After the training, the SOM neural network can quickly locate the fault AP and optimize the network according to the changes of the network environment. Simulation results reveal that the proposed scheme can efficiently locate the AP where interference happens and optimize the interference with an improved user experience. Copyright © 2016 John Wiley \& Sons, Ltd.},
year = {2017}
}

@article{iot2016,
  abbr={IEEE},
  title={Optimization Design and Application of Intelligent Parking Lot in Smart City},
  author={Hao, Yang and Zhang, Anqi and Yingtong, Dou},
  journal={Published in IEEE Third International Conference on Universal Village, 6-8 OCT, Japan, },
  year={2016}
}

